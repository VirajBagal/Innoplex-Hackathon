{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quoratextemb', 'innoplexusav']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "from keras.layers import Dense,Bidirectional,CuDNNLSTM,CuDNNGRU,Flatten,Conv1D,Embedding,Dropout,BatchNormalization\n",
    "from keras.layers import LSTM,GRU,Activation,GlobalAveragePooling1D,GlobalMaxPooling1D,concatenate,SpatialDropout1D,Input\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint,Callback, EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from imblearn.over_sampling import SMOTE\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# def load_embed(file):\n",
    "#     def get_coefs(word,*arr): \n",
    "#         return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "#     if file == '../input/quoratextemb/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n",
    "#         embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "#     else:\n",
    "#         embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "        \n",
    "#     return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragram_path='../input/quoratextemb/embeddings/paragram_300_sl999/paragram_300_sl999.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path='../input/quoratextemb/embeddings/glove.840B.300d/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_path='../input/quoratextemb/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf=pd.read_csv('../input/innoplexusav/train.csv')\n",
    "testdf=pd.read_csv('../input/innoplexusav/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe175bf21d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "traindf.sentiment.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_0=traindf[traindf['sentiment']==0].sample(n=1500,random_state=5,replace=True)\n",
    "# text_2=traindf[traindf['sentiment']==2].sample(n=1500,random_state=5)\n",
    "# text_1=traindf[traindf['sentiment']==1].sample(n=1500,random_state=5,replace=True)\n",
    "\n",
    "# traindf=pd.concat([text_0,text_1,text_2],axis=0,ignore_index=True)\n",
    "# traindf=traindf.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "ntrain=traindf.shape[0]\n",
    "data=pd.concat([traindf,testdf],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words=set(stopwords.words('english'))\n",
    "\n",
    "# def clean_text(row):\n",
    "#     tokens=word_tokenize(row)\n",
    "#     filtered_text=[word.lower() for word in tokens if ((word.lower() not in (string.punctuation + '‚Äú‚Äù‚Äô')) & (word.lower() not in stop_words))]\n",
    "#     return ' '.join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['text']=data['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    vocab={}\n",
    "    for sentence in tqdm(sentences):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word]+=1\n",
    "            except KeyError:\n",
    "                vocab[word]=1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences=data['text'].progress_apply(lambda x: x.split()).values\n",
    "# vocab=build_vocab(sentences)\n",
    "\n",
    "# print({k:vocab[k] for k in list(vocab.keys())[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def coverage(vocab,embedding_index):\n",
    "#     intersection={}\n",
    "#     i=0\n",
    "#     oov={}\n",
    "#     o=0\n",
    "    \n",
    "#     for word in tqdm(vocab.keys()):\n",
    "#         try:\n",
    "#             intersection[word]=embedding_index[word]\n",
    "#             i+=vocab[word]\n",
    "#         except:\n",
    "#             oov[word]=vocab[word]\n",
    "#             o+=vocab[word]\n",
    "#     print(f'Embeddings found for {len(intersection)*100/len(vocab)} of vocab')\n",
    "#     print(f'Embeddings found for {i*100/(i+o)} of text')\n",
    "#     oov=sorted(oov.items(),key=lambda x:x[1],reverse=True)\n",
    "#     return oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragram_oov=coverage(vocab,paragram_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lowered_text']=data['text'].apply(lambda x: x.lower())\n",
    "# sentences=data['lowered_text'].progress_apply(lambda x: x.split()).values\n",
    "# lowered_vocab=build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragram_oov=coverage(lowered_vocab,paragram_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8203/8203 [00:00<00:00, 25546.33it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8203/8203 [00:00<00:00, 12006.46it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences=data['text'].progress_apply(lambda x: x.split()).values\n",
    "vocab=build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lower(embedding, vocab):\n",
    "    count = 0\n",
    "    for word in vocab:\n",
    "        if word in embedding and word.lower() not in embedding:  \n",
    "            embedding[word.lower()] = embedding[word]\n",
    "            count += 1\n",
    "    print(f\"Added {count} words to embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_lower(paragram_index, vocab)\n",
    "# add_lower(glove_index,vocab)\n",
    "# add_lower(fasttext_index,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragram_oov=coverage(lowered_vocab,paragram_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
    "                       \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "                       \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n",
    "                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\",\n",
    "                       \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\",\n",
    "                       \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\",\n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\",\n",
    "                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
    "                       \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                       \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
    "                       \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
    "                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                       \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\",\n",
    "                       \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
    "                       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
    "                       \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "                       \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
    "                       \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
    "                       \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
    "                       \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
    "                       \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
    "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def known_contractions(embed):\n",
    "#     known = []\n",
    "#     for contract in contraction_mapping:\n",
    "#         if contract in embed:\n",
    "#             known.append(contract)\n",
    "#     return known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(known_contractions(paragram_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"‚Äô\", \"‚Äò\", \"¬¥\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['treated_text'] = data['lowered_text'].apply(lambda x: clean_contractions(x, contraction_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences=data['treated_text'].progress_apply(lambda x: x.split()).values\n",
    "# vocab = build_vocab(sentences)\n",
    "# oov_paragram = coverage(vocab, paragram_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"‚Äú‚Äù‚Äô' + '‚àûŒ∏√∑Œ±‚Ä¢√†‚àíŒ≤‚àÖ¬≥œÄ‚Äò‚Çπ¬¥¬∞¬£‚Ç¨\\√ó‚Ñ¢‚àö¬≤‚Äî‚Äì&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown_punct(embed, punct):\n",
    "    unknown = ''\n",
    "    for p in punct:\n",
    "        if p not in embed:\n",
    "            unknown += p\n",
    "            unknown += ' '\n",
    "    return unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(unknown_punct(paragram_index, punct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_mapping = {\"‚Äò\": \"'\", \"‚Çπ\": \"e\", \"¬¥\": \"'\", \"¬∞\": \"\", \"‚Ç¨\": \"e\", \"‚Ñ¢\": \"tm\", \"‚àö\": \" sqrt \", \"√ó\": \"x\", \"¬≤\": \"2\", \"‚Äî\": \"-\", \"‚Äì\": \"-\",\n",
    "                 \"‚Äô\": \"'\", \"_\": \"-\", \"`\": \"'\", '‚Äú': '\"', '‚Äù': '\"', '‚Äú': '\"', \"¬£\": \"e\", '‚àû': 'infinity', 'Œ∏': 'theta', '√∑': '/', \n",
    "                 'Œ±': 'alpha', '‚Ä¢': '.', '√†': 'a', '‚àí': '-', 'Œ≤': 'beta', '‚àÖ': '', '¬≥': '3', 'œÄ': 'pi', '‚ñ∫':' ','¬Æ':'registered trademark ',\n",
    "                 '√¢':'a','¬ª':' ','¬±':'+ -','√¢e':'e','1√¢':' ','‚â•':'> =','‚Ä°':'reference mark','üôÇ':'smile','2√¢':' ','¬∑':'-',\n",
    "                 '‚Ä†':'reference mark','rebif¬Æ':'rebif','¬©':'copyright trademark','‚â•6':'> = 6','gilenya¬Æ':'gilenya','¬ß':' ','8√¢':' ',\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_chars(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '‚Ä¶': ' ... ', '\\ufeff': '', '‡§ï‡§∞‡§®‡§æ': '', '‡§π‡•à': ''}  # Other special characters that I have to deal with in last\n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['treated_text'] = data['treated_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences=data['treated_text'].progress_apply(lambda x: x.split()).values\n",
    "# vocab = build_vocab(sentences)\n",
    "# oov_paragram = coverage(vocab, paragram_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oov_paragram[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['num_words']=data['text'].progress_apply(lambda x: len(x.split()))\n",
    "# data['num_sentences']=data['text'].progress_apply(lambda x: len(x.split('.')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_unique_words(text):\n",
    "#     tokens=word_tokenize(text)\n",
    "#     unique_words=[word for word in tokens if word.lower() not in paragram_index]\n",
    "#     return len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_punctuations(text):\n",
    "#     tokens=word_tokenize(text)\n",
    "#     punctuations=[word for word in tokens if word.lower() in punct]\n",
    "#     return len(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['num_unique_words']=data['text'].progress_apply(lambda x: count_unique_words(x) )\n",
    "# data['num_punctuation']=data['text'].progress_apply(lambda x: count_punctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def num_capital_letters(text):\n",
    "#     tokens=word_tokenize(text)\n",
    "#     capital=[word for word in tokens if word[0].isupper()]\n",
    "#     return len(capital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['num_capital']=data['text'].progress_apply(lambda x: num_capital_letters(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drug</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>lowered_text</th>\n",
       "      <th>treated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gilenya</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Autoimmune diseases tend to come in clusters. ...</td>\n",
       "      <td>2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0</td>\n",
       "      <td>autoimmune diseases tend to come in clusters. ...</td>\n",
       "      <td>autoimmune diseases tend to come in clusters ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gilenya</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I can completely understand why you‚Äôd want to ...</td>\n",
       "      <td>9eba8f80e7e20f3a2f48685530748fbfa95943e4</td>\n",
       "      <td>i can completely understand why you‚Äôd want to ...</td>\n",
       "      <td>i can completely understand why you would want...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fingolimod</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Interesting that it only targets S1P-1/5 recep...</td>\n",
       "      <td>fe809672251f6bd0d986e00380f48d047c7e7b76</td>\n",
       "      <td>interesting that it only targets s1p-1/5 recep...</td>\n",
       "      <td>interesting that it only targets s1p  -  1  / ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ocrevus</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Very interesting, grand merci. Now I wonder wh...</td>\n",
       "      <td>bd22104dfa9ec80db4099523e03fae7a52735eb6</td>\n",
       "      <td>very interesting, grand merci. now i wonder wh...</td>\n",
       "      <td>very interesting ,  grand merci .  now i wonde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gilenya</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hi everybody, My latest MRI results for Brain ...</td>\n",
       "      <td>b227688381f9b25e5b65109dd00f7f895e838249</td>\n",
       "      <td>hi everybody, my latest mri results for brain ...</td>\n",
       "      <td>hi everybody ,  my latest mri results for brai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         drug  sentiment                                               text  \\\n",
       "0     gilenya        2.0  Autoimmune diseases tend to come in clusters. ...   \n",
       "1     gilenya        2.0  I can completely understand why you‚Äôd want to ...   \n",
       "2  fingolimod        2.0  Interesting that it only targets S1P-1/5 recep...   \n",
       "3     ocrevus        2.0  Very interesting, grand merci. Now I wonder wh...   \n",
       "4     gilenya        1.0  Hi everybody, My latest MRI results for Brain ...   \n",
       "\n",
       "                                unique_hash  \\\n",
       "0  2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0   \n",
       "1  9eba8f80e7e20f3a2f48685530748fbfa95943e4   \n",
       "2  fe809672251f6bd0d986e00380f48d047c7e7b76   \n",
       "3  bd22104dfa9ec80db4099523e03fae7a52735eb6   \n",
       "4  b227688381f9b25e5b65109dd00f7f895e838249   \n",
       "\n",
       "                                        lowered_text  \\\n",
       "0  autoimmune diseases tend to come in clusters. ...   \n",
       "1  i can completely understand why you‚Äôd want to ...   \n",
       "2  interesting that it only targets s1p-1/5 recep...   \n",
       "3  very interesting, grand merci. now i wonder wh...   \n",
       "4  hi everybody, my latest mri results for brain ...   \n",
       "\n",
       "                                        treated_text  \n",
       "0  autoimmune diseases tend to come in clusters ....  \n",
       "1  i can completely understand why you would want...  \n",
       "2  interesting that it only targets s1p  -  1  / ...  \n",
       "3  very interesting ,  grand merci .  now i wonde...  \n",
       "4  hi everybody ,  my latest mri results for brai...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le=LabelEncoder()\n",
    "# data['drug']=le.fit_transform(data['drug'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols=['treated_text','num_words','num_sentences','num_punctuation','num_unique_words','num_capital','drug']\n",
    "train=data.loc[:ntrain-1,'text']\n",
    "target=data.loc[:ntrain-1,'sentiment'].astype(int)\n",
    "test=data.loc[ntrain: ,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target=to_categorical(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2924,) (2924, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test.shape, testdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Glad to hear it works for you. It was pretty m...\n",
       "1    Hi Kegles! I am so sorry you are dealing with ...\n",
       "2    Did the doctors say why they don't think you c...\n",
       "3    Thanks for the heads up Marco. This is very in...\n",
       "4    UPDATE: I saw IBD Specialist yesterday for my ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data(train,test):\n",
    "#     tk=Tokenizer()\n",
    "#     tk.fit_on_texts(list(train['treated_text'].values)+list(test['treated_text'].values))\n",
    "#     train['treated_text']=tk.texts_to_sequences(train['treated_text'])\n",
    "#     test['treated_text']=tk.texts_to_sequences(test['treated_text'])\n",
    "#     train_pad=pad_sequences(train['treated_text'],maxlen=max_len)\n",
    "#     test_pad=pad_sequences(test['treated_text'],maxlen=max_len)\n",
    "#     return train_pad, test_pad , tk.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_features=30000\n",
    "def get_data(train,test):\n",
    "    tk=Tokenizer()\n",
    "    tk.fit_on_texts(list(train.values)+list(test.values))\n",
    "    train=tk.texts_to_sequences(train)\n",
    "    test=tk.texts_to_sequences(test)\n",
    "    train_pad=pad_sequences(train,maxlen=max_len)\n",
    "    test_pad=pad_sequences(test,maxlen=max_len)\n",
    "    return train_pad, test_pad , tk.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pad,test_pad,word_index=get_data(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(vocab,word_index,file):\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    if file == '../input/quoratextemb/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "    \n",
    "    add_lower(embeddings_index, vocab)\n",
    "    embedding_stack=np.stack(embeddings_index.values())\n",
    "    embed_mean,embed_std=embedding_stack.mean(),embedding_stack.std()\n",
    "    embedding_matrix=np.random.normal(embed_mean,embed_std,(len(word_index),embedding_stack.shape[1]))\n",
    "    \n",
    "    for word,i in word_index.items():\n",
    "        if i>=len(word_index):\n",
    "            continue\n",
    "        embedding_vec=embeddings_index.get(word)\n",
    "        if embedding_vec is not None:\n",
    "            embedding_matrix[i]=embedding_vec\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train=train.drop('treated_text',axis=1)\n",
    "# test=test.drop('treated_text',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_embeddings(embedding_index):\n",
    "#     embedding_stack=np.stack(embedding_index.values())\n",
    "#     embed_mean,embed_std=embedding_stack.mean(),embedding_stack.std()\n",
    "#     embedding_matrix=np.random.normal(embed_mean,embed_std,(len(word_index),embedding_stack.shape[1]))\n",
    "    \n",
    "#     for word,i in word_index.items():\n",
    "#         if i>=len(word_index):\n",
    "#             continue\n",
    "#         embedding_vec=embedding_index.get(word)\n",
    "#         if embedding_vec is not None:\n",
    "#             embedding_matrix[i]=embedding_vec\n",
    "            \n",
    "#     return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1 words to embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:11: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2426 words to embedding\n"
     ]
    }
   ],
   "source": [
    "paragram_matrix=get_embeddings(vocab,word_index,paragram_path)\n",
    "glove_matrix=get_embeddings(vocab,word_index,glove_path)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix=np.concatenate((paragram_matrix,glove_matrix),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size=embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_cols=['num_words','num_sentences','num_punctuation','num_unique_words','num_capital','drug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5,restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(spatial_dr=0.5,units=128,dr=0,conv_size=3,kernel_size1=32,units2=64,lr=1e-3,lr_d=1e-10):\n",
    "    \n",
    "    \n",
    "#     text_input=Input(shape=(max_len,))\n",
    "#     feature_input=Input(shape=(6,))\n",
    "#     embeddings=Embedding(embedding_matrix.shape[0],embed_size,weights=[embedding_matrix],trainable=False)(text_input)\n",
    "#     x=SpatialDropout1D(spatial_dr)(embeddings)\n",
    "#     lstm=Bidirectional(CuDNNLSTM(units,return_sequences=True,kernel_initializer='he_uniform'))(x)\n",
    "#     conv1d=Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(lstm)\n",
    "#     global_max=GlobalMaxPooling1D()(conv1d)\n",
    "#     dense1=Dense(units2)(feature_input)\n",
    "#     concat=concatenate([global_max,dense1])\n",
    "#     dense2=Dense(units)(concat)\n",
    "#     x=Dropout(dr)(dense2)\n",
    "#     batch_norm=BatchNormalization()(x)\n",
    "#     out=Dense(3,activation='softmax')(batch_norm)\n",
    "#     model=Model(inputs=[text_input,feature_input],outputs=out)\n",
    "#     model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=lr,decay=lr_d),metrics=[f1])\n",
    "  \n",
    "#     return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model1(gru_units=64,lstm_units=128):\n",
    "    inp=Input(shape=(max_len,))\n",
    "    embed=Embedding(embedding_matrix.shape[0],embed_size,weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x=SpatialDropout1D(0.5)(embed)\n",
    "#     gru=CuDNNGRU(gru_units,return_sequences=True)(x)\n",
    "    lstm=CuDNNLSTM(lstm_units,return_sequences=True,activity_regularizer=l1(0.001))(x)\n",
    "    gap1=GlobalAveragePooling1D()(lstm)\n",
    "#     gap2=GlobalAveragePooling1D()(lstm)\n",
    "#     concat=concatenate([gap1,gap2])\n",
    "    out=Dense(3,activation='softmax')(gap1)\n",
    "    model=Model(inputs=inp,outputs=out)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=[f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2(gru_units=128):\n",
    "    inp=Input(shape=(max_len,))\n",
    "    embed=Embedding(embedding_matrix.shape[0],embed_size,weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x=SpatialDropout1D(0.5)(embed)\n",
    "    gru=CuDNNGRU(gru_units,return_sequences=True)(x)\n",
    "    gap1=GlobalAveragePooling1D()(gru)\n",
    "    out=Dense(3,activation='softmax')(gap1)\n",
    "    model=Model(inputs=inp,outputs=out)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=[f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model3(gru_units=32,lstm_units=32):\n",
    "    inp=Input(shape=(max_len,))\n",
    "    embed=Embedding(embedding_matrix.shape[0],embed_size,weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x=SpatialDropout1D(0.5)(embed)\n",
    "    gru=CuDNNGRU(gru_units,return_sequences=True)(x)\n",
    "    lstm=CuDNNLSTM(lstm_units,return_sequences=True)(gru)\n",
    "    gap1=GlobalAveragePooling1D()(gru)\n",
    "    gap2=GlobalAveragePooling1D()(lstm)\n",
    "    concat=concatenate([gap1,gap2])\n",
    "    out=Dense(3,activation='softmax')(gap1)\n",
    "    model=Model(inputs=inp,outputs=out)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=[f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.initializers import *\n",
    "from keras.optimizers import *\n",
    "import keras.backend as K\n",
    "from keras.callbacks import *\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import re\n",
    "from keras.regularizers import l1\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.RandomUniform(seed=10000)\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model4(gru_units=32):\n",
    "    inp=Input(shape=(max_len,))\n",
    "    embed=Embedding(embedding_matrix.shape[0],embed_size,weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x=SpatialDropout1D(0.5)(embed)\n",
    "    gru=CuDNNGRU(gru_units,return_sequences=True,activity_regularizer=l1(0.001))(x)\n",
    "    last = Lambda(lambda t: t[:, -1], name='last')(gru)\n",
    "    maxpool = GlobalMaxPooling1D()(gru)\n",
    "    attn = AttentionWeightedAverage()(gru)\n",
    "    c = concatenate([last, maxpool, attn], axis=1)\n",
    "    c = Reshape((3, -1))(c)\n",
    "    c = Lambda(lambda x:K.sum(x, axis=1))(c)\n",
    "    x = BatchNormalization()(c)\n",
    "    x = Dense(200, activation='relu', kernel_initializer=glorot_uniform(seed=111000))(x)\n",
    "    x = Dropout(0.2, seed=1024)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    output_layer = Dense(3, activation=\"softmax\")(x)\n",
    "    model=Model(inputs=inp,outputs=output_layer)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=[f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model5(spatialdropout=0.2, rnn_units=128, filters=[100, 80, 30, 12], weight_decay=0.10):\n",
    "    K.clear_session()       \n",
    "    inp = Input(shape=(max_len,))\n",
    "    x = Embedding(embedding_matrix.shape[0], embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(rate=spatialdropout, seed=10000)(x)\n",
    "    x = CuDNNLSTM(rnn_units, return_sequences=True, \n",
    "                               kernel_initializer=glorot_uniform(seed=111000), \n",
    "                               recurrent_initializer=Orthogonal(gain=1.0, seed=123000))(x)\n",
    "\n",
    "    x1 = Conv1D(filters=filters[0], activation='relu', kernel_size=1, \n",
    "                padding='same', kernel_initializer=glorot_uniform(seed=110000))(x)\n",
    "    x2 = Conv1D(filters=filters[1], activation='relu', kernel_size=2, \n",
    "                padding='same', kernel_initializer=glorot_uniform(seed=120000))(x)\n",
    "    x3 = Conv1D(filters=filters[2], activation='relu', kernel_size=3, \n",
    "                padding='same', kernel_initializer=glorot_uniform(seed=130000))(x)\n",
    "    x4 = Conv1D(filters=filters[3], activation='relu', kernel_size=5, \n",
    "                padding='same', kernel_initializer=glorot_uniform(seed=140000))(x)\n",
    "\n",
    "    \n",
    "    x1 = GlobalMaxPool1D()(x1)\n",
    "    x2 = GlobalMaxPool1D()(x2)\n",
    "    x3 = GlobalMaxPool1D()(x3)\n",
    "    x4 = GlobalMaxPool1D()(x4)\n",
    "\n",
    "    c = concatenate([x1, x2, x3, x4])\n",
    "    x = Dense(200, activation='relu', kernel_initializer=glorot_uniform(seed=111000))(c)\n",
    "    x = Dropout(0.2, seed=10000)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(3, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=[f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Train on 10326 samples, validate on 529 samples\n",
      "Epoch 1/1\n",
      "10326/10326 [==============================] - 6s 592us/step - loss: 192.5236 - f1: 0.0016 - val_loss: 37.1037 - val_f1: 0.0000e+00\n",
      "2924/2924 [==============================] - 0s 134us/step\n",
      "Validation f1 score is : 0.22783647179601121\n",
      "Fold: 2\n",
      "Train on 10326 samples, validate on 529 samples\n",
      "Epoch 1/1\n",
      "10326/10326 [==============================] - 3s 251us/step - loss: 42.6659 - f1: 0.0000e+00 - val_loss: 16.5884 - val_f1: 0.0000e+00\n",
      "2924/2924 [==============================] - 0s 132us/step\n",
      "Validation f1 score is : 0.11649403702195364\n",
      "Fold: 3\n",
      "Train on 10326 samples, validate on 529 samples\n",
      "Epoch 1/1\n",
      "10326/10326 [==============================] - 3s 250us/step - loss: 20.6327 - f1: 0.0000e+00 - val_loss: 8.1830 - val_f1: 0.0000e+00\n",
      "2924/2924 [==============================] - 0s 134us/step\n",
      "Validation f1 score is : 0.09881122094464528\n",
      "Fold: 4\n",
      "Train on 10326 samples, validate on 529 samples\n",
      "Epoch 1/1\n",
      "10326/10326 [==============================] - 3s 250us/step - loss: 10.2177 - f1: 0.0000e+00 - val_loss: 3.8758 - val_f1: 0.0000e+00\n",
      "2924/2924 [==============================] - 0s 133us/step\n",
      "Validation f1 score is : 0.10362735698306168\n",
      "Fold: 5\n",
      "Train on 10326 samples, validate on 529 samples\n",
      "Epoch 1/1\n",
      "10326/10326 [==============================] - 3s 250us/step - loss: 4.9846 - f1: 0.0000e+00 - val_loss: 2.2950 - val_f1: 0.0000e+00\n",
      "2924/2924 [==============================] - 0s 133us/step\n",
      "Validation f1 score is : 0.12317204759065224\n",
      "Fold: 6\n",
      "Train on 10329 samples, validate on 528 samples\n",
      "Epoch 1/1\n",
      "10329/10329 [==============================] - 3s 249us/step - loss: 2.6196 - f1: 0.0000e+00 - val_loss: 1.5982 - val_f1: 0.0000e+00\n",
      "1216/2924 [===========>..................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924/2924 [==============================] - 0s 133us/step\n",
      "Validation f1 score is : 0.0905757984705353\n",
      "Fold: 7\n",
      "Train on 10329 samples, validate on 528 samples\n",
      "Epoch 1/1\n",
      "10329/10329 [==============================] - 3s 250us/step - loss: 1.7114 - f1: 0.0000e+00 - val_loss: 1.3538 - val_f1: 0.0000e+00\n",
      "1216/2924 [===========>..................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924/2924 [==============================] - 0s 132us/step\n",
      "Validation f1 score is : 0.07005649717514124\n",
      "Fold: 8\n",
      "Train on 10329 samples, validate on 526 samples\n",
      "Epoch 1/1\n",
      "10329/10329 [==============================] - 3s 249us/step - loss: 1.4278 - f1: 0.0000e+00 - val_loss: 1.2735 - val_f1: 0.0000e+00\n",
      "1216/2924 [===========>..................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924/2924 [==============================] - 0s 132us/step\n",
      "Validation f1 score is : 0.09085933223864258\n",
      "Fold: 9\n",
      "Train on 10329 samples, validate on 526 samples\n",
      "Epoch 1/1\n",
      "10329/10329 [==============================] - 3s 249us/step - loss: 1.3539 - f1: 0.0000e+00 - val_loss: 1.2568 - val_f1: 0.0000e+00\n",
      "1216/2924 [===========>..................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924/2924 [==============================] - 0s 135us/step\n",
      "Validation f1 score is : 0.06927881885292449\n",
      "Fold: 10\n",
      "Train on 10329 samples, validate on 526 samples\n",
      "Epoch 1/1\n",
      "10329/10329 [==============================] - 3s 250us/step - loss: 1.3309 - f1: 0.0000e+00 - val_loss: 1.2633 - val_f1: 0.0000e+00\n",
      "1216/2924 [===========>..................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924/2924 [==============================] - 0s 133us/step\n",
      "Validation f1 score is : 0.10077139162616489\n",
      "****************************************************************************************************\n",
      "Mean of all validation f1 scores: 0.10914829726997326\n",
      "****************************************************************************************************\n",
      "Fold: 1\n",
      "Train on 10326 samples, validate on 529 samples\n",
      "Epoch 1/1\n",
      "10326/10326 [==============================] - 4s 378us/step - loss: 783.8074 - f1: 0.3008 - val_loss: 187.7432 - val_f1: 0.2279\n",
      "2924/2924 [==============================] - 0s 134us/step\n",
      "Validation f1 score is : 0.32468107477595143\n",
      "Fold: 2\n",
      "Train on 10326 samples, validate on 529 samples\n",
      "Epoch 1/1\n",
      "10326/10326 [==============================] - 3s 242us/step - loss: 236.6431 - f1: 0.2429 - val_loss: 81.9789 - val_f1: 0.0220\n",
      "2924/2924 [==============================] - 0s 133us/step\n",
      "Validation f1 score is : 0.1775945105059029\n",
      "Fold: 3\n",
      "Train on 10326 samples, validate on 529 samples\n",
      "Epoch 1/1\n",
      "10326/10326 [==============================] - 2s 241us/step - loss: 98.5182 - f1: 0.2145 - val_loss: 26.0051 - val_f1: 0.0000e+00\n",
      "1280/2924 [============>.................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924/2924 [==============================] - 0s 131us/step\n",
      "Validation f1 score is : 0.13455263939136966\n",
      "Fold: 4\n",
      "Train on 10326 samples, validate on 529 samples\n",
      "Epoch 1/1\n",
      "10326/10326 [==============================] - 2s 241us/step - loss: 27.8799 - f1: 0.1720 - val_loss: 12.3708 - val_f1: 0.0000e+00\n",
      "1216/2924 [===========>..................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924/2924 [==============================] - 0s 131us/step\n",
      "Validation f1 score is : 0.0837381488525976\n",
      "Fold: 5\n",
      "Train on 10326 samples, validate on 529 samples\n",
      "Epoch 1/1\n",
      "10326/10326 [==============================] - 2s 241us/step - loss: 11.1800 - f1: 0.1717 - val_loss: 8.1104 - val_f1: 0.0000e+00\n",
      "1216/2924 [===========>..................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924/2924 [==============================] - 0s 132us/step\n",
      "Validation f1 score is : 0.3171176011349613\n",
      "Fold: 6\n",
      "Train on 10329 samples, validate on 528 samples\n",
      "Epoch 1/1\n",
      "10329/10329 [==============================] - 2s 239us/step - loss: 8.3589 - f1: 0.1231 - val_loss: 6.3908 - val_f1: 0.0038\n",
      "2924/2924 [==============================] - 0s 132us/step\n",
      "Validation f1 score is : 0.27814569536423844\n",
      "Fold: 7\n",
      "Train on 10329 samples, validate on 528 samples\n",
      "Epoch 1/1\n",
      "10329/10329 [==============================] - 2s 241us/step - loss: 7.2797 - f1: 0.1120 - val_loss: 5.1797 - val_f1: 0.0000e+00\n",
      "2924/2924 [==============================] - 0s 132us/step\n",
      "Validation f1 score is : 0.16741866381041637\n",
      "Fold: 8\n",
      "Train on 10329 samples, validate on 526 samples\n",
      "Epoch 1/1\n",
      "10329/10329 [==============================] - 2s 239us/step - loss: 6.6648 - f1: 0.0910 - val_loss: 5.2437 - val_f1: 0.0038\n",
      "2924/2924 [==============================] - 0s 132us/step\n",
      "Validation f1 score is : 0.2853743219045917\n",
      "Fold: 9\n",
      "Train on 10329 samples, validate on 526 samples\n",
      "Epoch 1/1\n",
      "10329/10329 [==============================] - 2s 241us/step - loss: 6.6457 - f1: 0.0918 - val_loss: 5.4500 - val_f1: 0.0150\n",
      "2924/2924 [==============================] - 0s 132us/step\n",
      "Validation f1 score is : 0.2304569188900608\n",
      "Fold: 10\n",
      "Train on 10329 samples, validate on 526 samples\n",
      "Epoch 1/1\n",
      "10329/10329 [==============================] - 2s 241us/step - loss: 6.3143 - f1: 0.0879 - val_loss: 4.8765 - val_f1: 0.0224\n",
      "2924/2924 [==============================] - 0s 130us/step\n",
      "Validation f1 score is : 0.3035226599583035\n",
      "****************************************************************************************************\n",
      "Mean of all validation f1 scores: 0.2302602234588394\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "n_folds=10\n",
    "skf=StratifiedKFold(n_splits=n_folds,random_state=5,shuffle=True)\n",
    "oof_model1=np.zeros((train_pad.shape[0],3))\n",
    "oof_model4=np.zeros((train_pad.shape[0],3))\n",
    "test_pred_model1=np.zeros((test_pad.shape[0],3))\n",
    "test_pred_model4=np.zeros((test_pad.shape[0],3))\n",
    "smote=SMOTE(random_state=5)\n",
    "scores_df=[]\n",
    "units=[(32,32)]\n",
    "\n",
    "\n",
    "for model in [build_model1(32),build_model4(128)]:\n",
    "    all_scores=[]\n",
    "    for fold,(train_idx,val_idx) in enumerate(skf.split(train_pad,target)):\n",
    "        print('Fold:',fold+1)\n",
    "    \n",
    "        ohe_target=to_categorical(target)\n",
    "        train_X,val_X=train_pad[train_idx],train_pad[val_idx]\n",
    "        train_y,val_y=ohe_target[train_idx],ohe_target[val_idx]\n",
    "    \n",
    "        train_Xsampled,train_ysampled=smote.fit_sample(train_X,train_y)\n",
    "        model=model\n",
    "        history=model.fit(train_Xsampled,train_ysampled,batch_size=128,epochs=1,verbose=1,validation_data=(val_X,val_y))\n",
    "    \n",
    "        val_pred=np.argmax(model.predict(val_X,batch_size=64),axis=1)\n",
    "        if model==build_model1(32):\n",
    "            oof_model1[val_idx]=model.predict(val_X,batch_size=64)\n",
    "            true_y=np.argmax(val_y,axis=1)\n",
    "            score=f1_score(true_y,val_pred,average='macro')\n",
    "            pred=model.predict(test_pad,batch_size=64,verbose=1)\n",
    "            test_pred_model1+=pred/n_folds\n",
    "        else:\n",
    "            oof_model4[val_idx]=model.predict(val_X,batch_size=64)\n",
    "            true_y=np.argmax(val_y,axis=1)\n",
    "            score=f1_score(true_y,val_pred,average='macro')\n",
    "            pred=model.predict(test_pad,batch_size=64,verbose=1)\n",
    "            test_pred_model4+=pred/n_folds\n",
    "        all_scores.append(score)\n",
    "        print('Validation f1 score is :',score)\n",
    "    print('*'*100)\n",
    "    print('Mean of all validation f1 scores:',np.mean(all_scores))\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof=0.7*oof_model4 + 0.3*oof_model1\n",
    "oof=np.argmax(oof,axis=1)\n",
    "\n",
    "test_pred=0.7*test_pred_model4 + 0.3*test_pred_model1\n",
    "predictions=np.argmax(test_pred,axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1=np.zeros((len(test_pred),3),dtype=np.float32)\n",
    "vector1=[8.56,6.31,1.38]\n",
    "for i in range(len(test_pred)):\n",
    "    for j in range(3):\n",
    "        predictions1[i][j]=vector1[j]*test_pred[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2=np.zeros((len(test_pred),3), dtype=np.float32)\n",
    "vector2=[7.56,5.31,1.38]\n",
    "for i in range(len(test_pred)):\n",
    "    for j in range(3):\n",
    "        predictions2[i][j]=vector2[j]*test_pred[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions3=np.zeros((len(test_pred),3), dtype=np.float32)\n",
    "vector3=[7.56,5.31,1.38]\n",
    "for i in range(len(test_pred)):\n",
    "    for j in range(3):\n",
    "        predictions3[i][j]=vector3[j]*test_pred[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions4=np.zeros((len(test_pred),3), dtype=np.float32)\n",
    "vector4=[5,3.5,1.38]\n",
    "for i in range(len(test_pred)):\n",
    "    for j in range(3):\n",
    "        predictions4[i][j]=vector4[j]*test_pred[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2924\n",
       "dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions1=pd.Series(np.argmax(predictions1,axis=1).astype(int))\n",
    "predictions1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2924\n",
       "dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2=pd.Series(np.argmax(predictions2,axis=1).astype(int))\n",
    "predictions2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2924\n",
       "dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions3=pd.Series(np.argmax(predictions3,axis=1).astype(int))\n",
    "predictions3.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2924\n",
       "dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions4=pd.Series(np.argmax(predictions4,axis=1).astype(int))\n",
    "predictions4.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_predictions=np.argmax(test_pred_model1,axis=1)\n",
    "model4_predictions=np.argmax(test_pred_model4,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gru_df=pd.DataFrame(scores_df,columns=['Units','Epochs','Scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_df=pd.DataFrame(scores_df,columns=['Units','Epochs','Scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gru_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of all validation f1 scores: 0.3141029416408934   GRU 64 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_X,val_X,train_y,val_y=train_test_split(train_pad,target,test_size=0.1,random_state=5,stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "#     file_path = \"best_model.hdf5\"\n",
    "#     check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "#                                   save_best_only = True, mode = \"min\")\n",
    "#     early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5,restore_best_weights=True)\n",
    "    \n",
    "# #     feat_inp = Input(shape = (6,))\n",
    "#     text_inp=Input(shape=(max_len,))\n",
    "#     text_embedding = Embedding(embedding_matrix.shape[0], embed_size,weights=[embedding_matrix],trainable=False)(text_inp)\n",
    "# #     drug_embedding= Embedding(train_drug.nunique()+1, drug_embed_size,input_length=1)(drug_inp)\n",
    "# #     dense1=Dense(dense_units*2)(feat_inp)\n",
    "    \n",
    "#     x1 = SpatialDropout1D(spatial_dr)(text_embedding)\n",
    "# #     x1=concatenate([x1,dense1])\n",
    "#     x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "#     x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "#     avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
    "#     max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "#     x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "#     avg_pool3_gru = GlobalAveragePooling1D()(x3)\n",
    "#     max_pool3_gru = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "#     x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "#     x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "#     avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
    "#     max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "#     x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "#     avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n",
    "#     max_pool3_lstm = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    \n",
    "#     x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n",
    "#                     avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "#     x = Dense(3, activation = \"softmax\")(x)\n",
    "#     model = Model(inputs = text_inp, outputs = x)\n",
    "#     model.compile(loss = \"categorical_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [f1])\n",
    "#     history = model.fit(train_Xsampled, train_ysampled, batch_size = 128, epochs = 50, validation_data=(val_X,val_y), \n",
    "#                         verbose = 1, callbacks = [check_point, early_stop])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1=build_model()\n",
    "# history=model1.fit([train_pad,train],target,epochs=50,batch_size=64,validation_split=0.1,verbose=1,\n",
    "#                    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe1666220f0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "oof=pd.Series(oof)\n",
    "oof.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fdcbba80320>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions=pd.Series(predictions)\n",
    "predictions.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1594\n",
       "0    1029\n",
       "1     301\n",
       "dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fdcbba63208>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADlhJREFUeJzt3U+MXWd5x/HvD5vQqqDGNJPI2G5twVTFWWDQyERiQ0mbOOnCQSqSswALRTILRwKJRQ2bUGgkkAqRkCCSUSxMRXGtAopFrabGpUIsIJ7QNMRxU09Digdb8VCHAEJK6/Tp4r4WN/Z45s4fzxDe70e6Ouc85zn3vGfj35x/16kqJEn9edVqD0CStDoMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKn1q72AOZyww031ObNm1d7GJL0ivLYY4/9pKrG5uv7tQ6AzZs3Mzk5udrDkKRXlCT/NUrfvJeAkvxWkkeT/FuSk0n+stW3JPlektNJ/i7Jda3+mrY81dZvHvquj7T600luX9yhSZKWwyj3AF4E3lVVbwG2ATuS3AJ8CnigqsaB54F7Wv89wPNV9SbggdZHkq3ALuBmYAfw+SRrlvNgJEmjmzcAauAXbfHV7VPAu4C/b/WDwF1tfmdbpq2/NUla/VBVvVhVPwSmgO3LchSSpAUb6SmgJGuSPA6cB44B/wn8tKoutpZpYEOb3wCcAWjrXwB+b7g+yzbD+9qTZDLJ5MzMzMKPSJI0kpECoKpeqqptwEYGf7W/eba2Ns1V1l2tfvm+9lfVRFVNjI3NexNbkrRIC3oPoKp+CvwLcAtwfZJLTxFtBM62+WlgE0Bb/7vAheH6LNtIklbYKE8BjSW5vs3/NvAnwCngW8Cft7bdwMNt/khbpq3/5xr8t2NHgF3tKaEtwDjw6HIdiCRpYUZ5D2A9cLA9sfMq4HBVfSPJU8ChJH8F/CvwUOt/CPibJFMM/vLfBVBVJ5McBp4CLgJ7q+ql5T0cSdKo8uv8fwJPTEyUL4Itn837/mG1hyBd1bOf/LPVHsJvjCSPVdXEfH3+FpAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROzRsASTYl+VaSU0lOJvlgq38syY+TPN4+dw5t85EkU0meTnL7UH1Hq00l2XdtDkmSNIq1I/RcBD5cVd9P8jrgsSTH2roHquqvh5uTbAV2ATcDbwC+meQP2+rPAX8KTAMnkhypqqeW40AkSQszbwBU1TngXJv/eZJTwIY5NtkJHKqqF4EfJpkCtrd1U1X1DECSQ63XAJCkVbCgewBJNgNvBb7XSvcmeSLJgSTrWm0DcGZos+lWu1r98n3sSTKZZHJmZmYhw5MkLcDIAZDktcBXgQ9V1c+AB4E3AtsYnCF8+lLrLJvXHPWXF6r2V9VEVU2MjY2NOjxJ0gKNcg+AJK9m8I//l6vqawBV9dzQ+i8A32iL08Cmoc03Amfb/NXqkqQVNspTQAEeAk5V1WeG6uuH2t4NPNnmjwC7krwmyRZgHHgUOAGMJ9mS5DoGN4qPLM9hSJIWapQzgHcA7wV+kOTxVvsocHeSbQwu4zwLfACgqk4mOczg5u5FYG9VvQSQ5F7gEWANcKCqTi7jsUiSFmCUp4C+w+zX74/Osc39wP2z1I/OtZ0kaeX4JrAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROzRsASTYl+VaSU0lOJvlgq78+ybEkp9t0XasnyWeTTCV5Isnbhr5rd+s/nWT3tTssSdJ8RjkDuAh8uKreDNwC7E2yFdgHHK+qceB4Wwa4Axhvnz3AgzAIDOA+4O3AduC+S6EhSVp58wZAVZ2rqu+3+Z8Dp4ANwE7gYGs7CNzV5ncCX6qB7wLXJ1kP3A4cq6oLVfU8cAzYsaxHI0ka2YLuASTZDLwV+B5wU1Wdg0FIADe2tg3AmaHNplvtanVJ0ioYOQCSvBb4KvChqvrZXK2z1GqO+uX72ZNkMsnkzMzMqMOTJC3QSAGQ5NUM/vH/clV9rZWfa5d2aNPzrT4NbBrafCNwdo76y1TV/qqaqKqJsbGxhRyLJGkBRnkKKMBDwKmq+szQqiPApSd5dgMPD9Xf154GugV4oV0iegS4Lcm6dvP3tlaTJK2CtSP0vAN4L/CDJI+32keBTwKHk9wD/Ah4T1t3FLgTmAJ+CbwfoKouJPkEcKL1fbyqLizLUUiSFmzeAKiq7zD79XuAW2fpL2DvVb7rAHBgIQOUJF0bvgksSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHVq3gBIciDJ+SRPDtU+luTHSR5vnzuH1n0kyVSSp5PcPlTf0WpTSfYt/6FIkhZilDOALwI7Zqk/UFXb2ucoQJKtwC7g5rbN55OsSbIG+BxwB7AVuLv1SpJWydr5Gqrq20k2j/h9O4FDVfUi8MMkU8D2tm6qqp4BSHKo9T614BFLkpbFUu4B3JvkiXaJaF2rbQDODPVMt9rV6ldIsifJZJLJmZmZJQxPkjSXxQbAg8AbgW3AOeDTrZ5ZemuO+pXFqv1VNVFVE2NjY4scniRpPvNeAppNVT13aT7JF4BvtMVpYNNQ60bgbJu/Wl2StAoWdQaQZP3Q4ruBS08IHQF2JXlNki3AOPAocAIYT7IlyXUMbhQfWfywJUlLNe8ZQJKvAO8EbkgyDdwHvDPJNgaXcZ4FPgBQVSeTHGZwc/cisLeqXmrfcy/wCLAGOFBVJ5f9aCRJIxvlKaC7Zyk/NEf//cD9s9SPAkcXNDpJ0jXjm8CS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6NW8AJDmQ5HySJ4dqr09yLMnpNl3X6kny2SRTSZ5I8rahbXa3/tNJdl+bw5EkjWqUM4AvAjsuq+0DjlfVOHC8LQPcAYy3zx7gQRgEBnAf8HZgO3DfpdCQJK2OeQOgqr4NXLisvBM42OYPAncN1b9UA98Frk+yHrgdOFZVF6rqeeAYV4aKJGkFLfYewE1VdQ6gTW9s9Q3AmaG+6Va7Wl2StEqW+yZwZqnVHPUrvyDZk2QyyeTMzMyyDk6S9CuLDYDn2qUd2vR8q08Dm4b6NgJn56hfoar2V9VEVU2MjY0tcniSpPksNgCOAJee5NkNPDxUf197GugW4IV2iegR4LYk69rN39taTZK0StbO15DkK8A7gRuSTDN4mueTwOEk9wA/At7T2o8CdwJTwC+B9wNU1YUknwBOtL6PV9XlN5YlSSto3gCoqruvsurWWXoL2HuV7zkAHFjQ6CRJ14xvAktSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnVpSACR5NskPkjyeZLLVXp/kWJLTbbqu1ZPks0mmkjyR5G3LcQCSpMVZjjOAP66qbVU10Zb3Acerahw43pYB7gDG22cP8OAy7FuStEjX4hLQTuBgmz8I3DVU/1INfBe4Psn6a7B/SdIIlhoABfxTkseS7Gm1m6rqHECb3tjqG4AzQ9tOt9rLJNmTZDLJ5MzMzBKHJ0m6mrVL3P4dVXU2yY3AsST/PkdvZqnVFYWq/cB+gImJiSvWS5KWx5LOAKrqbJueB74ObAeeu3Rpp03Pt/ZpYNPQ5huBs0vZvyRp8RYdAEl+J8nrLs0DtwFPAkeA3a1tN/Bwmz8CvK89DXQL8MKlS0WSpJW3lEtANwFfT3Lpe/62qv4xyQngcJJ7gB8B72n9R4E7gSngl8D7l7BvSdISLToAquoZ4C2z1P8buHWWegF7F7s/SdLy8k1gSeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnVrxAEiyI8nTSaaS7Fvp/UuSBlY0AJKsAT4H3AFsBe5OsnUlxyBJGljpM4DtwFRVPVNV/wMcAnau8BgkSax8AGwAzgwtT7eaJGmFrV3h/WWWWr2sIdkD7GmLv0jy9DUflbQ4NwA/We1B/KbIp1Z7BL9R/mCUppUOgGlg09DyRuDscENV7Qf2r+SgpMVIMllVE6s9DmmxVvoS0AlgPMmWJNcBu4AjKzwGSRIrfAZQVReT3As8AqwBDlTVyZUcgyRpIFU1f5ekKyTZ0y5ZSq9IBoAkdcqfgpCkThkAktSplX4MVHpFSvJHDN5a38Dg3ZWzwJGqOrWqA5OWwDMAaR5J/oLBz5YEeJTB48wBvuIPGuqVzJvA0jyS/Adwc1X972X164CTVTW+OiOTlsYzAGl+/we8YZb6+rZOekXyHoA0vw8Bx5Oc5lc/Zvj7wJuAe1dtVNISeQlIGkGSVzH4OfMNDK7/TwMnquqlVR2YtAQGgCR1ynsAktQpA0CSOmUASFKnDABJ6pQBIEmd+n9V44dulP8HMAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1_predictions=pd.Series(model1_predictions)\n",
    "model1_predictions.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2924\n",
       "dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_predictions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fdcbb9c4c50>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model4_predictions=pd.Series(model4_predictions)\n",
    "model4_predictions.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1594\n",
       "0    1029\n",
       "1     301\n",
       "dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4_predictions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=pd.read_csv('../input/innoplexusav/sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['sentiment']=predictions\n",
    "sub.to_csv('my_submission.csv',index=False)\n",
    "\n",
    "sub['sentiment']=predictions1\n",
    "sub.to_csv('my_submission1.csv',index=False)\n",
    "\n",
    "sub['sentiment']=predictions2\n",
    "sub.to_csv('my_submission2.csv',index=False)\n",
    "\n",
    "sub['sentiment']=predictions3\n",
    "sub.to_csv('my_submission3.csv',index=False)\n",
    "\n",
    "sub['sentiment']=predictions4\n",
    "sub.to_csv('my_submission4.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
